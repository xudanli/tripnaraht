#!/usr/bin/env python3
"""
åŸå¸‚æ•°æ®å¯¼å…¥è„šæœ¬
ç›´æ¥è¯»å–æ•°æ®æ–‡ä»¶ï¼ˆCSV/Excel/JSONï¼‰å¹¶å¯¼å…¥åˆ° PostgreSQL æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•:
    python import_cities.py <æ•°æ®æ–‡ä»¶è·¯å¾„>

ç¤ºä¾‹:
    python import_cities.py cities.csv
    python import_cities.py cities.xlsx
    python import_cities.py cities.json
"""

import sys
import pandas as pd
import json
import psycopg2
from psycopg2.extras import execute_values
from psycopg2.extensions import register_adapter, AsIs
from pathlib import Path
from typing import Dict, Any, Optional, List
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ•°æ®åº“è¿æ¥é…ç½®
DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'port': int(os.getenv('DB_PORT', 5432)),
    'database': os.getenv('DB_NAME', 'postgres'),
    'user': os.getenv('DB_USER', 'postgres'),
    'password': os.getenv('DB_PASSWORD', ''),
}

# å­—æ®µæ˜ å°„ï¼šåŸå§‹å­—æ®µå -> æ•°æ®åº“å­—æ®µå
FIELD_MAPPING = {
    # å¿…éœ€å­—æ®µ
    'NAME': 'name',
    'name': 'name',
    'city': 'name',
    'cityName': 'name',
    'åŸå¸‚åç§°': 'name',
    
    'ISO_A2': 'countryCode',
    'countryCode': 'countryCode',
    'country_code': 'countryCode',
    'iso_code': 'countryCode',
    'å›½å®¶ä»£ç ': 'countryCode',
    
    # å¯é€‰å­—æ®µ
    'NAME_ZH': 'nameCN',
    'NAME_ZHT': 'nameCN',
    'nameCN': 'nameCN',
    'name_zh': 'nameCN',
    'ä¸­æ–‡åç§°': 'nameCN',
    
    'NAME_EN': 'nameEN',
    'nameEN': 'nameEN',
    'name_en': 'nameEN',
    'è‹±æ–‡åç§°': 'nameEN',
    
    'çº¬åº¦': 'latitude',
    'LAT': 'latitude',
    'latitude': 'latitude',
    'lat': 'latitude',
    'y': 'latitude',
    
    'ç»åº¦': 'longitude',
    'LNG': 'longitude',
    'LON': 'longitude',
    'longitude': 'longitude',
    'lng': 'longitude',
    'lon': 'longitude',
    'x': 'longitude',
    
    'TIMEZONE': 'timezone',
    'TIMEZO': 'timezone',
    'timezone': 'timezone',
    'timeZone': 'timezone',
    'æ—¶åŒº': 'timezone',
    
    'adcode': 'adcode',
    'ad_code': 'adcode',
    'admin_code': 'adcode',
    'è¡Œæ”¿åŒºåˆ’ä»£ç ': 'adcode',
}


def read_data_file(file_path: str) -> pd.DataFrame:
    """è¯»å–æ•°æ®æ–‡ä»¶"""
    file_path = Path(file_path)
    
    if not file_path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    print(f"ğŸ“‚ è¯»å–æ–‡ä»¶: {file_path}")
    
    suffix = file_path.suffix.lower()
    
    if suffix == '.csv':
        df = pd.read_csv(file_path, encoding='utf-8')
    elif suffix in ['.xlsx', '.xls']:
        df = pd.read_excel(file_path)
    elif suffix == '.json':
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if isinstance(data, list):
            df = pd.DataFrame(data)
        elif isinstance(data, dict) and 'cities' in data:
            df = pd.DataFrame(data['cities'])
        else:
            raise ValueError("JSON æ ¼å¼é”™è¯¯ï¼šåº”ä¸ºæ•°ç»„æˆ–åŒ…å« 'cities' æ•°ç»„çš„å¯¹è±¡")
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")
    
    print(f"âœ… è¯»å–æˆåŠŸï¼Œå…± {len(df)} è¡Œï¼Œ{len(df.columns)} åˆ—\n")
    return df


def extract_field_value(row: pd.Series, field_names: List[str]) -> Optional[Any]:
    """ä»è¡Œæ•°æ®ä¸­æå–å­—æ®µå€¼ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰"""
    for field in field_names:
        if field in row and pd.notna(row[field]):
            value = row[field]
            # å¤„ç†å­—ç¬¦ä¸²ç±»å‹ï¼Œå»é™¤é¦–å°¾ç©ºæ ¼
            if isinstance(value, str):
                return value.strip()
            return value
    return None


def convert_row_to_city_data(row: pd.Series) -> Optional[Dict[str, Any]]:
    """å°†æ•°æ®è¡Œè½¬æ¢ä¸º City è¡¨æ•°æ®æ ¼å¼"""
    # å¿…éœ€å­—æ®µ
    name = extract_field_value(row, ['NAME', 'name', 'city', 'cityName', 'åŸå¸‚åç§°'])
    country_code = extract_field_value(row, ['ISO_A2', 'countryCode', 'country_code', 'iso_code', 'å›½å®¶ä»£ç '])
    
    if not name or not country_code:
        return None
    
    # éªŒè¯å›½å®¶ä»£ç æ ¼å¼
    country_code = str(country_code).strip().upper()
    if len(country_code) != 2 or not country_code.isalpha():
        return None
    
    city_data = {
        'name': str(name).strip(),
        'countryCode': country_code,
    }
    
    # å¯é€‰å­—æ®µ
    name_cn = extract_field_value(row, ['NAME_ZH', 'NAME_ZHT', 'nameCN', 'name_zh', 'ä¸­æ–‡åç§°'])
    if name_cn:
        city_data['nameCN'] = str(name_cn).strip()
    
    name_en = extract_field_value(row, ['NAME_EN', 'nameEN', 'name_en', 'è‹±æ–‡åç§°'])
    if name_en:
        city_data['nameEN'] = str(name_en).strip()
    
    # åæ ‡
    lat = extract_field_value(row, ['çº¬åº¦', 'LAT', 'latitude', 'lat', 'y'])
    lng = extract_field_value(row, ['ç»åº¦', 'LNG', 'LON', 'longitude', 'lng', 'lon', 'x'])
    
    if lat is not None and lng is not None:
        try:
            city_data['latitude'] = float(lat)
            city_data['longitude'] = float(lng)
        except (ValueError, TypeError):
            pass
    
    # æ—¶åŒº
    timezone = extract_field_value(row, ['TIMEZONE', 'TIMEZO', 'timezone', 'timeZone', 'æ—¶åŒº'])
    if timezone and len(str(timezone)) > 3:
        city_data['timezone'] = str(timezone).strip()
    
    # è¡Œæ”¿åŒºåˆ’ä»£ç 
    adcode = extract_field_value(row, ['adcode', 'ad_code', 'admin_code', 'è¡Œæ”¿åŒºåˆ’ä»£ç '])
    if adcode:
        adcode_str = str(adcode).strip()
        if adcode_str.isdigit() and len(adcode_str) == 6:
            city_data['adcode'] = adcode_str
    
    # Metadataï¼ˆæ‰©å±•ä¿¡æ¯ï¼‰
    metadata = {}
    
    # è¡Œæ”¿åŒºåˆ’
    adm0 = extract_field_value(row, ['ADM0NAME', 'country', 'å›½å®¶'])
    if adm0:
        metadata['adminLevel0'] = str(adm0).strip()
    
    adm1 = extract_field_value(row, ['ADM1NAME', 'province', 'state', 'çœ', 'å·'])
    if adm1:
        metadata['adminLevel1'] = str(adm1).strip()
    
    # å¤–éƒ¨ID
    wikidata_id = extract_field_value(row, ['WIKIDATAID', 'WIKID/A', 'wikidataId'])
    if wikidata_id:
        metadata['wikidataId'] = str(wikidata_id).strip()
    
    geonames_id = extract_field_value(row, ['GEONAMESID', 'geonamesId'])
    if geonames_id:
        try:
            metadata['geonamesId'] = int(geonames_id)
        except (ValueError, TypeError):
            metadata['geonamesId'] = str(geonames_id).strip()
    
    wof_id = extract_field_value(row, ['WOF_ID', 'wofId'])
    if wof_id:
        try:
            metadata['wofId'] = int(wof_id)
        except (ValueError, TypeError):
            metadata['wofId'] = str(wof_id).strip()
    
    # å…¶ä»–è¯­è¨€åç§°
    lang_fields = {
        'NAME_DE': 'nameDE',
        'NAME_ES': 'nameES',
        'NAME_FR': 'nameFR',
        'NAME_JA': 'nameJA',
        'NAME_KO': 'nameKO',
    }
    
    for source_field, target_key in lang_fields.items():
        value = extract_field_value(row, [source_field])
        if value:
            metadata[target_key] = str(value).strip()
    
    # è¦ç´ åˆ†ç±»
    feature_class = extract_field_value(row, ['FEATURECLA', 'featureClass'])
    if feature_class:
        metadata['featureClass'] = str(feature_class).strip()
    
    if metadata:
        city_data['metadata'] = json.dumps(metadata, ensure_ascii=False)
    
    return city_data


def import_cities_to_db(cities_data: List[Dict[str, Any]], batch_size: int = 500):
    """å¯¼å…¥åŸå¸‚æ•°æ®åˆ°æ•°æ®åº“"""
    print(f"ğŸ”Œ è¿æ¥æ•°æ®åº“: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}")
    
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    
    try:
        success_count = 0
        skipped_count = 0
        error_count = 0
        
        print(f"\nğŸ“Š å¼€å§‹å¯¼å…¥ {len(cities_data)} æ¡æ•°æ®...\n")
        
        for i, city in enumerate(cities_data, 1):
            try:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                cur.execute("""
                    SELECT id FROM "City" 
                    WHERE name = %s AND "countryCode" = %s
                """, (city['name'], city['countryCode']))
                
                existing = cur.fetchone()
                if existing:
                    skipped_count += 1
                    if i % 100 == 0:
                        print(f"è¿›åº¦: {i}/{len(cities_data)} (å·²å­˜åœ¨: {skipped_count}, æˆåŠŸ: {success_count}, é”™è¯¯: {error_count})")
                    continue
                
                # æ„å»º SQL
                fields = ['name', '"countryCode"']
                values = [city['name'], city['countryCode']]
                placeholders = ['%s', '%s']
                
                if 'nameCN' in city:
                    fields.append('"nameCN"')
                    values.append(city['nameCN'])
                    placeholders.append('%s')
                
                if 'nameEN' in city:
                    fields.append('"nameEN"')
                    values.append(city['nameEN'])
                    placeholders.append('%s')
                
                if 'timezone' in city:
                    fields.append('timezone')
                    values.append(city['timezone'])
                    placeholders.append('%s')
                
                if 'adcode' in city:
                    fields.append('adcode')
                    values.append(city['adcode'])
                    placeholders.append('%s')
                
                if 'metadata' in city:
                    fields.append('metadata')
                    values.append(city['metadata'])
                    placeholders.append('%s::jsonb')
                
                # æ„å»º SQL å’Œå‚æ•°
                if 'latitude' in city and 'longitude' in city:
                    # æœ‰åæ ‡ï¼Œä½¿ç”¨ PostGIS å‡½æ•°
                    fields.append('location')
                    placeholders.append('ST_SetSRID(ST_MakePoint(%s, %s), 4326)')
                    # åæ ‡å€¼éœ€è¦å•ç‹¬æ·»åŠ 
                    sql = f"""
                        INSERT INTO "City" ({', '.join(fields)})
                        VALUES ({', '.join(placeholders)})
                        RETURNING id
                    """
                    # æ„å»ºæœ€ç»ˆå‚æ•°åˆ—è¡¨ï¼Œåæ ‡æ”¾åœ¨æœ€å
                    final_values = values.copy()
                    final_values.extend([city['longitude'], city['latitude']])
                    cur.execute(sql, final_values)
                else:
                    # æ²¡æœ‰åæ ‡ï¼Œæ™®é€šæ’å…¥
                    sql = f"""
                        INSERT INTO "City" ({', '.join(fields)})
                        VALUES ({', '.join(placeholders)})
                        RETURNING id
                    """
                    cur.execute(sql, values)
                
                city_id = cur.fetchone()[0]
                success_count += 1
                
                # æ¯ batch_size æ¡æäº¤ä¸€æ¬¡ï¼Œå¹¶æ˜¾ç¤ºè¿›åº¦
                if i % batch_size == 0:
                    conn.commit()
                    progress = (i / len(cities_data) * 100)
                    print(f"è¿›åº¦: {i}/{len(cities_data)} ({progress:.1f}%) - å·²å­˜åœ¨: {skipped_count}, æˆåŠŸ: {success_count}, é”™è¯¯: {error_count}")
                elif i % 100 == 0:
                    # æ¯ 100 æ¡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦ï¼ˆä½†ä¸æäº¤ï¼‰
                    progress = (i / len(cities_data) * 100)
                    print(f"è¿›åº¦: {i}/{len(cities_data)} ({progress:.1f}%) - å·²å­˜åœ¨: {skipped_count}, æˆåŠŸ: {success_count}, é”™è¯¯: {error_count}")
                
            except Exception as e:
                error_count += 1
                print(f"âŒ å¯¼å…¥å¤±è´¥: {city.get('name', 'Unknown')} - {str(e)}")
                conn.rollback()
        
        # æœ€ç»ˆæäº¤
        conn.commit()
        
        print(f"\n{'='*50}")
        print(f"ğŸ“Š å¯¼å…¥å®Œæˆ:")
        print(f"  âœ… æˆåŠŸåˆ›å»º: {success_count}")
        print(f"  â­ï¸  å·²å­˜åœ¨ï¼ˆè·³è¿‡ï¼‰: {skipped_count}")
        print(f"  âŒ å¤±è´¥: {error_count}")
        print(f"{'='*50}\n")
        
    finally:
        cur.close()
        conn.close()


def main():
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python import_cities.py <æ•°æ®æ–‡ä»¶è·¯å¾„>")
        print("\næ”¯æŒçš„æ–‡ä»¶æ ¼å¼:")
        print("  - CSV (.csv)")
        print("  - Excel (.xlsx, .xls)")
        print("  - JSON (.json)")
        sys.exit(1)
    
    file_path = sys.argv[1]
    
    try:
        # è¯»å–æ•°æ®
        df = read_data_file(file_path)
        
        # è½¬æ¢æ•°æ®
        print("ğŸ”„ è½¬æ¢æ•°æ®...")
        cities_data = []
        skipped = []
        
        for idx, row in df.iterrows():
            city_data = convert_row_to_city_data(row)
            if city_data:
                cities_data.append(city_data)
            else:
                skipped.append(idx + 1)
        
        print(f"âœ… è½¬æ¢å®Œæˆ: {len(cities_data)} æ¡æœ‰æ•ˆæ•°æ®")
        if skipped:
            print(f"â­ï¸  è·³è¿‡: {len(skipped)} æ¡ï¼ˆç¼ºå°‘å¿…éœ€å­—æ®µï¼‰")
        
        if not cities_data:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯å¯¼å…¥")
            sys.exit(1)
        
        # å¯¼å…¥æ•°æ®åº“
        import_cities_to_db(cities_data)
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()

